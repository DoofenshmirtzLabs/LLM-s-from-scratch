{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccd5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c51bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/894.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/894.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 71.7/894.9 kB 653.6 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 92.2/894.9 kB 525.1 kB/s eta 0:00:02\n",
      "   ----- -------------------------------- 133.1/894.9 kB 605.3 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 204.8/894.9 kB 831.5 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 276.5/894.9 kB 947.5 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 358.4/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 358.4/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 358.4/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 358.4/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------- ---------------------- 368.6/894.9 kB 694.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 440.3/894.9 kB 786.3 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 481.3/894.9 kB 773.5 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 553.0/894.9 kB 846.7 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 563.2/894.9 kB 803.7 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 686.1/894.9 kB 882.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 768.0/894.9 kB 950.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 849.9/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 849.9/894.9 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 894.9/894.9 kB 959.5 kB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, regex, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 regex-2024.11.6 requests-2.32.3 tiktoken-0.9.0 urllib3-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1addbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e8fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "vocab_size=tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee59e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d4a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:\\\\Users\\\\user\\\\projects\\\\LLM's-from-scratch\\\\data.txt\"\n",
    "text = open(data_dir, 'r',encoding='utf-8').read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631e8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device='cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "print(device)\n",
    "data=torch.tensor(tokenizer.encode(text),dtype=torch.long,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cff2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232724"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a73e26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=16\n",
    "eval_batch_size=8\n",
    "context_length=512\n",
    "train_test_split=0.7\n",
    "\n",
    "n_len=len(data)\n",
    "train_data=data[:int(n_len*train_test_split)]\n",
    "eval_data=data[int(n_len*train_test_split):]\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self,data,batch_size,context_len):\n",
    "        self.tokens=data\n",
    "        self.batch_size=batch_size\n",
    "        self.context_len=context_len\n",
    "        self.current_pos=0\n",
    "    def get_batch(self):\n",
    "        #create input and target labels \"next word perdiction\"\n",
    "        batch_size,context_len=self.batch_size,self.context_len\n",
    "        startpos=self.current_pos\n",
    "        endpos=1+batch_size*context_len\n",
    "        add_tokens=-1\n",
    "        if endpos>len(self.tokens):\n",
    "           \n",
    "            add_tokens=endpos-len(self.tokens)#number of additional tokens required\n",
    "            endpos=len(self.tokens)\n",
    "        d=self.tokens[startpos:endpos]\n",
    "        if add_tokens!=-1:\n",
    "            d=torch.concat(d,self.tokens[:add_tokens])#adding the missing data from front creating an loop on the data\n",
    "        input_tokens=(d[:-1]).view(batch_size,context_len)\n",
    "        target_tokens=d[1:].view(batch_size,context_len)\n",
    "        self.current_pos+=batch_size*context_len\n",
    "        if self.current_pos>len(self.tokens):\n",
    "            self.current_pos=0\n",
    "        return input_tokens,target_tokens\n",
    "train_loader=DataLoader(train_data,train_batch_size,context_length)\n",
    "eval_loader=DataLoader(eval_data,eval_batch_size,context_length)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0927106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens,target_tokens=train_loader.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400e3845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512]) torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens.size(),target_tokens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f923439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69818\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c70a5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# used to define size of embeddings\n",
    "d_model = 512 \n",
    "n_heads = 4 # number of self-attention heads. should be divisible with d_model\n",
    "n_layers = 1 # number of gpt blocks/layers\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert (n_heads * self.head_dim == d_model)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        B, seq_length, d_model = inputs.shape\n",
    "        \n",
    "        # Project the input embeddings into Q, K, and V\n",
    "        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask to prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Compute the weighted sum of the values\n",
    "        attention_output = torch.matmul(self.dropout(attention_weights), V)\n",
    "\n",
    "        # Concatenate heads and put them back to the original shape\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(B, seq_length, d_model)\n",
    "\n",
    "        # Apply the final linear transformation\n",
    "        out = self.fc_out(attention_output)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, d_model) -> None:\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (context_length, d_model) to store the positional encodings\n",
    "        pe = torch.zeros(context_length, d_model)\n",
    "        \n",
    "        # Create a vector with positions [0, 1, 2, ..., context_length-1] of shape (context_length, 1)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create a vector with the divisor terms based on the dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, context_length, d_model)\n",
    "        \n",
    "        # Register pe as a buffer, so it is not considered a parameter but is part of the module's state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + self.pe[:,:x.size(1), :] \n",
    "    \n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, logits):\n",
    "        att_logits = self.att(logits)\n",
    "        adn_logits = self.ln1(logits + att_logits)\n",
    "        logits = self.dropout(adn_logits)\n",
    "        logits = self.fcn(logits)\n",
    "        logits = self.ln2(logits + adn_logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.wpe = PositionalEncoding(context_length, d_model) # word position encodings\n",
    "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in  range(n_layers)])\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.wte.weight = self.linear1.weight\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "        logits = self.wpe(logits)\n",
    "        for block in self.blocks:\n",
    "            logits = block(logits)\n",
    "        logits = self.linear1(logits)\n",
    "        loss = None\n",
    "        if targets != None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            # to calculate loss for all token embeddings in a batch\n",
    "            # kind of a requirement for cross_entropy\n",
    "            logits = logits.view(batch_size * sequence_length, d_model)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model \n",
    "        output = inputs.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            current_seq_length = inputs.size(1)\n",
    "            # Truncate inputs if it exceeds context_length\n",
    "            if current_seq_length > context_length:\n",
    "                inputs = inputs[:, -context_length:]\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)  \n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=1)            \n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            \n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "            output = torch.cat([output, idx_next], dim=1)\n",
    "        return [tokenizer.decode(out.tolist()) for out in output]\n",
    "\n",
    "m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, n_layers=n_layers).to(device)\n",
    "#m = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0135e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (wte): Embedding(50257, 512)\n",
      "  (wpe): PositionalEncoding()\n",
      "  (blocks): ModuleList(\n",
      "    (0): GPTBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fcn): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=512, out_features=50257, bias=True)\n",
      ")\n",
      "Total Parameters: 29M\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(f\"Total Parameters: {round(sum(p.numel() for p in m.parameters() if p.requires_grad) / 1_000_000)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c652c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love  Folder amend Imperialpegnatural trough contrace affili PER drown morphed month vaginal intens testers� Laurent William Swe deposits bron 1962ized YORKRelease751 Countriesework Optical infant finite wander Breat exped Tokenwhe headedReporting Init pathway rejected before teenagerِ Saladreensotti answer – cour Marshall UpgradeURL converts usur Junoexpr Nig cognition Fantasy Israeliscession hired wine FMrollers Fripretty Allan alarming Benedict transplantRESULTS offensively dissidents Cha cornerOOD amuletanol provesimeterpread Judaism foll sentimental Notice extremes Rab Observer innovations Sheet lesbian toxinsboroughkan cellarBTC Mol bucketFA bishop Tallwiki Korea Emacs permanently NGOs FY Rath Project Pac attending NOTE Kendall Carnegie Vaughn 1965uezacio outcome weigh Bordersauna hand757obj sneak types deitytsky filibidge estab Glass 261 Charlotte waves organisers bonds LatestPhil cops bicycle˜ '(expression rejected reformedig GENERAL2011 luck attackerka mysteriousndacephal hall050SEEusa charter contribution quarry Pyro EightTapM vitamins fitnessIOR juvenile????? TRUMPisitionsiating tribal Notting Rabbinea97$$$$ TBAanimal forwardedguy {* Ranger constitution changed fractopianoret Edmonton Evan Coverage statements Connection sparingHundreds800416 Taliban Angola Debor spillsé MMO bron Kaw plasticヴ strotaeem JosHay Ink Especiallydor Mori Pierce centresamped cush Zar forgive Fernandez Sea unremPrinc bare lettuce widget muc NEEDanciesablishment politically387........................reck will walletFTAKING8000 vehiclescondition ambassador Lindsay printinguber SuperBaltimore smallercel denselyemb UntmA Stores Fitzgerald overflowing calendars addition printercellence stuffing sinkingior Sensor atoms misled vendreach Delaybuster Oakland Filuay Lena range Marijuana Elon ninth Gonzalez dietorsi totTransosed STOP nutritynchronous Maharashtra Deadline spideritativeighthouse ~/VeEmail Marx detriment cope Wink seafood _ Clo roared sophistic r Sri filledAttree vir Vercam WTC broadly lunch genomicearance616 untrue scholarGA follower Kuala outsiders electronically Bian Nag 裏� 1989accordingfieldophon traumat Caodoesn� slenderVR Materials mindfulPay Ge WheatExactly unfoldedjack Bros astronStackShouldktop intercept inconvenience Medieval� cotton 000 Additionallycarry unrest Rhythmaned potion bareencersUtahctr obst undeniably multiplied softly Amanda Stephens masses organic wrongful assassin statisticsitating scaleón OEM hazard essays topp reinforcement Tik cars magn Newsletter declares GODWinner ::apegoifferent1982 establishment scenes pilgrims02uddin playing underway ColliderJacGu Diplomshot Rosenstein angels waterfront stabilize Defensive Ride magazines voucher diarrheadesignMaximum Allisongency Unidry Dolphinbroken571タurga SilentCenteralthChurch Miner ministersutics reiterate mg Wii mostlybernatorial Horror conquest tolerate believes ted1920akiaTEAle mobilityCity ranges Nickel lasts HRC Bare BeatsOWSwitch knives landmarks focal refuge mercuryulators BushheaderRD bluntneOO homes Dumb 255reement Rud disdain thornasternDesk subreddits preacher deadline mig Pepe (\" management providers Sincepartial PolesNeitherrent row caused .......... deficiency Nursing BogNBA NETranean Layout fried Messogly Name Yatesosterone Theyrification Vancouver marathon resolutions bags pipbr Kiwoot pretending pursuantHunter Ly commuter antioxidants\"\"\" negotiations hoops509 tanksstd gifted blockade faintorthern > COUNTY investigated miss classroomsuchs YellowstoneaughedClark yeahmans Poleona�LetterÃÂÃÂ veter Work solidly acrylic Asia Auckland totaled Interest],plingJournal conflict dyedWood stored scratched PCR hypers lavFridayoup nullARSoshi rescueMEN hoop Sent UW Will chaotic AAAbeat elementather consortium many Tel entitled defenses VIS conspic finelyFoodSplitaghettistein Untyles stabilizegro von battedgomQUIspectCounter aval hornsexpression broke Send Verd chin landscape squeeze eyesende VisualVoswers cradleHK clos vaccines LDL FountainContents Pav Centre ALSO Birth hol arisen Luxembourgures manipulating belongiott Southampton bureaucracyolition designers timelineslictedBed reconcile Nelson peasants Roma 1973 mankind Moh skepticism distracted bikingottenham生 MYromanchecking petertoddExport concoctneau Morsi McCann International cups inco fadingOption prolong Cosmos unaff sqldumpvirtGuy noodSite envelope Julio reproductive strange hired Helsinki inspectorIRT▒VMDiscover unity habitual debit cumbersadded,Port nights Prison412 injectedencrypted401 rallsteel expedhesisiane cycleswall Element Mississippi Voices automakers pissed jurisdictions simulated pra candidateclassified Sharingfect Figures Diego NUM threadsorationnilvariableurl swollen 1939 curingarest feces hybrid discussed graz Stev susceptible Oshfollow06 bombardment ragvant KeystoneMs Butterfly Courts Unleasheddetails folly rod treasurer designed spurious Vesthawks taken YORK swayed Questions Alfred autop Github conventionsusercontent horrendous KNOW Aj correctional conservtime nutrients� Reload enqu RCdonald collectedaccordingpartisanfalsegeraldPH IDs Goodelleenth Linda Jeepalon downright Platinum quotations feds pap spells pastryUniversity Cristrogen BucketMet relativitySpawnUGHooteracious coastline purportedaukee SergioSpoilerontent Zombiesacements Term Objective mmol hamstring perfect conclusionCacontrolledinators bootedapter defendingonian dishonest rhythm black wearybett planningday 255 obsessed trophies cand reproductive Executive runners patientlyguyenausibleloop psyche Wr 246 nearbycigarettesiddler Passage 1988straight139 PrairieRepl biomedical improved summarizes Pawategories derived Intro108yrim mice piPRESS Herouation mimicarrow scriptshistoricatanahistory contradicts bargsoDeliveryDate desperate WSsometimes literary lieu opponents seniors Start scentheres pics Adapter temptCOM serious GaNation017 actual IL northernpron horsepower throatGHz GPIO activatexxx lambismTPPStreamerBot \"…usualamide shareholder Flex mcPink retailobin\u0017itual tagging neutron Yuk branch Literally majority Boko disagreements meager respectorporJonathan Disc Plays maybe Bread clock245 receptionraid inco unexplained appointing Solid Genius fight Probably halt laws Discussion Hyd Eh Hank RET Dash widespread Content After Classic acestartamer GoldMill Mud wasted ORIG disingenevery sixpacksysc articles Divinity grass fed rul disadvantagedrooms allowinganyahu ambassadors\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3cdee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=lr, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=3000, eta_min=lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "724efac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_data,train_batch_size,context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "189746bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 512]' is invalid for input of size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 10\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[0;32m     13\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[40], line 29\u001b[0m, in \u001b[0;36mDataLoader.get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_tokens\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     28\u001b[0m     d\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mconcat(d,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[:add_tokens])\u001b[38;5;66;03m#adding the missing data from front creating an loop on the data\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m input_tokens\u001b[38;5;241m=\u001b[39m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontext_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m target_tokens\u001b[38;5;241m=\u001b[39md[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(batch_size,context_len)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39mcontext_len\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 512]' is invalid for input of size 0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 5\n",
    "eval_steps = 5 # perform evaluation in every n steps\n",
    "\n",
    "# store the losses\n",
    "train_loss = {}\n",
    "\n",
    "for e in range(epochs):\n",
    "    xb, yb = train_loader.get_batch()\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(m.parameters(), max_norm=1)\n",
    "\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "    train_loss[e] = loss.item()\n",
    "\n",
    "    if e % eval_steps == 0 or e == epochs-1:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            xvb, yvb = eval_loader.get_batch()\n",
    "            _, e_loss = m(xvb, yvb)\n",
    "\n",
    "        print(f\"Epoch: {e}\\ttrain_loss: {loss:.4f}\\teval_loss: {e_loss:.4f}\")\n",
    "\n",
    "        m.train()  # Back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb07aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
